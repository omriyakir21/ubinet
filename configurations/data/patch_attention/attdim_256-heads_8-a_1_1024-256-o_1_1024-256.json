{
    "hypothesis": "patch_attention",
    "experiment": "attdim_256-heads_8-a_1_1024-256-o_1_1024-256",
    "data": {
        "path": "/home/iscb/wolfson/doririmon/home/order/ubinet/repo/ubinet/datasets/patch_to_score/data_for_training/03_04_with_pesto_and_coord/folds_training_dicts.pkl",
        "use_pesto": true,
        "ablation_string": "111111111",
        "max_folds": null,
        "use_coordinates": true
    },
    "model": {
        "name": "patch_attention",
        "kwargs": {
            "output_mlp_hidden_sizes": [
                [
                    1024,
                    256
                ]
            ],
            "output_mlp_dropout_rate": 0.25,
            "attention_mlp_hidden_sizes": [
                [
                    1024,
                    256
                ]
            ],
            "attention_mlp_dropout_rate": 0.25,
            "activation": "relu",
            "input_shape": [
                10,
                9
            ],
            "max_number_of_patches": 10,
            "attention_dimension": 256,
            "num_heads": 8
        }
    },
    "compile": {
        "optimizer": {
            "name": "adamw",
            "kwargs": {
                "learning_rate": 0.001
            }
        },
        "loss": {
            "name": "binary_cross_entropy",
            "kwargs": {}
        }
    },
    "fit": {
        "epochs": 200,
        "batch_size": 256,
        "verbose": 1,
        "n_early_stopping_epochs": 12
    }
}