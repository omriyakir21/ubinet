{
    "hypothesis": "pts_transformer_encoder_mlps",
    "experiment": "mlp_a_dropout_0.0_mlp_c_dropout_0.2_hidden_sizes_mlp_a_[(1024, 1024), (1024, 1024)]_hidden_sizes_mlp_c_[(1024, 1024), (1024, 1024)]",
    "data": {
        "path": "/home/iscb/wolfson/doririmon/home/order/ubinet/repo/ubinet/datasets/patch_to_score/data_for_training/03_04_with_pesto/folds_training_dicts.pkl",
        "use_pesto": true,
        "ablation_string": "111111111",
        "max_folds": null
    },
    "model": {
        "name": "pts_transformer_encoder_mlp",
        "kwargs": {
            "hidden_sizes_mlp_a": [
                [
                    1024,
                    1024
                ],
                [
                    1024,
                    1024
                ]
            ],
            "mlp_a_dropout_rate": 0.0,
            "hidden_sizes_mlp_c": [
                [
                    1024,
                    1024
                ],
                [
                    1024,
                    1024
                ]
            ],
            "mlp_c_dropout_rate": 0.2,
            "activation": "relu",
            "input_shape": [
                10,
                9
            ],
            "max_number_of_patches": 10
        }
    },
    "compile": {
        "optimizer": {
            "name": "adamw",
            "kwargs": {
                "learning_rate": 0.001
            }
        },
        "loss": {
            "name": "binary_cross_entropy",
            "kwargs": {}
        }
    },
    "fit": {
        "epochs": 200,
        "batch_size": 256,
        "verbose": 1,
        "n_early_stopping_epochs": 12
    }
}