{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from utils import multi_exp_data_to_fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypothesis = 'pesto_protein_scores_ablation'\n",
    "hypothesis = 'optimizer_batchsize_lr_gridsearch'\n",
    "evaluation_type = 'validation'  # can be train / validation / test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fetch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = os.listdir(f'../configurations/data/{hypothesis}')\n",
    "experiments = [e.split('.json')[0] for e in experiments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iterate over all experiments, per experiment - sort runs by time\n",
    "# Going from most recent to oldest, take the first run of the expeirment that finished running (has inference results over all 5 folds)\n",
    "\n",
    "run_paths = []\n",
    "for experiment in experiments:\n",
    "    experiment_runs = os.listdir(f'../results/patch_to_score/hypotheses/{hypothesis}/{experiment}')\n",
    "    try:\n",
    "        experiment_runs = sorted(experiment_runs, key=lambda x: datetime.strptime(x.split('_')[0], '%Y-%m-%d'), reverse=True)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    experiment_valid_run_found = False\n",
    "    for updated_experiment_run in experiment_runs:\n",
    "        run_path = f'{hypothesis}/{experiment}/{updated_experiment_run}'\n",
    "        if 'fold_4' in os.listdir(f'../results/patch_to_score/hypotheses/{run_path}'):\n",
    "            run_paths.append(run_path)\n",
    "            experiment_valid_run_found = True\n",
    "            break\n",
    "    if not experiment_valid_run_found:\n",
    "        print(f'experiment: {experiment} didnt finish, skipping')\n",
    "len(run_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_paths = [f'../results/patch_to_score/hypotheses/{path}' for path in run_paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/36 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:20<00:00,  1.76it/s]\n"
     ]
    }
   ],
   "source": [
    "multi_exp_data = dict()\n",
    "for base_path in tqdm(base_paths):\n",
    "    exp_data = []\n",
    "    exp_name = base_path.split('/')[-2]\n",
    "    for fold in range(5):\n",
    "        try:\n",
    "            fold_labels = np.load(os.path.join(base_path, f'fold_{fold}', evaluation_type, 'labels.npy'))\n",
    "            fold_predictions = np.load(os.path.join(base_path, f'fold_{fold}', evaluation_type, 'predictions.npy'))[:, 0]\n",
    "            exp_data.append(np.stack([fold_labels, fold_predictions]))\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    multi_exp_data[exp_name] = exp_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_multi_exp_data = deepcopy(multi_exp_data)\n",
    "filtered_multi_exp_data = {exp_name: exp_data for exp_name, exp_data in filtered_multi_exp_data.items() if ('sgd' not in exp_name and '0.0001' not in exp_name)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing rocs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 42.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding random roc\n",
      "fig update\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "fig = multi_exp_data_to_fig(filtered_multi_exp_data, \n",
    "                            show_std_in_legend=True, log_scale=True, resolution=10_000)\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ubinet-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
