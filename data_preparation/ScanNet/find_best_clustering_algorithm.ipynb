{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'yaml' has no attribute 'error' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcsgraph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m connected_components\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m silhouette_score\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdata_preparation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mScanNet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcath_utils\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcath_utils\u001b[39;00m\n",
      "File \u001b[0;32m~/ubinet/data_preparation/ScanNet/../../data_preparation/ScanNet/cath_utils.py:11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcsgraph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m connected_components\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata_preparation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mScanNet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLabelPropagationAlgorithm_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m split_receptors_into_individual_chains_only_PSSM\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata_preparation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mScanNet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mAF2_augmentations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_chain_dict_with_all_info_only_pssm\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpaths\u001b[39;00m\n",
      "File \u001b[0;32m~/ubinet/data_preparation/ScanNet/../../data_preparation/ScanNet/LabelPropagationAlgorithm_utils.py:10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mScanNet_Ub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_FASTA, num2seq\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpdb\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata_preparation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mScanNet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdb_creation_scanNet_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save_as_pickle\n",
      "File \u001b[0;32m~/ubinet/data_preparation/ScanNet/../../models/ScanNet_Ub/preprocessing/sequence_utils.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prange, njit\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterpolate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interp1d\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/experiment/lib/python3.10/site-packages/numba/__init__.py:69\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m get_versions\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m generate_version_info\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types, errors\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Re-export typeof\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/experiment/lib/python3.10/site-packages/numba/core/config.py:10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# YAML needed to use file based Numba config\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01myaml\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     _HAVE_YAML \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/experiment/lib/python3.10/site-packages/yaml/__init__.py:13\u001b[0m\n\u001b[1;32m     11\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m6.0.2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcyaml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     14\u001b[0m     __with_libyaml__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/experiment/lib/python3.10/site-packages/yaml/cyaml.py:7\u001b[0m\n\u001b[1;32m      2\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCBaseLoader\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCSafeLoader\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCFullLoader\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUnsafeLoader\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCLoader\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCBaseDumper\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCSafeDumper\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCDumper\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      5\u001b[0m ]\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01myaml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_yaml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CParser, CEmitter\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstructor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mserializer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32myaml/_yaml.pyx:15\u001b[0m, in \u001b[0;36minit yaml._yaml\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'yaml' has no attribute 'error' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(), '..','..'))\n",
    "import paths\n",
    "from data_preparation.ScanNet.db_creation_scanNet_utils import load_as_pickle,save_as_pickle\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.cluster import SpectralClustering, AffinityPropagation\n",
    "from community import community_louvain  # Louvain\n",
    "from leidenalg import find_partition, ModularityVertexPartition  # Leiden\n",
    "import igraph as ig\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "from sklearn.metrics import silhouette_score\n",
    "import data_preparation.ScanNet.cath_utils as cath_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(468, 468)\n"
     ]
    }
   ],
   "source": [
    "dataset_date = \"8_9\"\n",
    "seq_id = \"0.95\"\n",
    "ASA_THRESHOLD_VALUE = 0.2\n",
    "with_scanNet = False\n",
    "with_scanNet_addition = '_with_scanNet_' if with_scanNet else ''\n",
    "matHomologous_path = os.path.join(paths.cath_intermediate_files_path, f'matHomologous_{dataset_date}{with_scanNet_addition}.pkl')\n",
    "matHomologous = load_as_pickle(matHomologous_path)\n",
    "print(matHomologous.shape)\n",
    "graphHomologous = csr_matrix(matHomologous) \n",
    "clusters_folder = os.path.join(paths.cath_intermediate_files_path, f'clusters{with_scanNet_addition}')\n",
    "os.makedirs(clusters_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_labels_by_cluster_size(labels):\n",
    "    \"\"\"Reorder labels such that label 0 is for the biggest cluster, label 1 for the second biggest, and so on.\"\"\"\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    sorted_indices = np.argsort(-counts)  # Sort in descending order\n",
    "    new_labels = np.zeros_like(labels)\n",
    "    for new_label, old_label in enumerate(unique[sorted_indices]):\n",
    "        new_labels[labels == old_label] = new_label\n",
    "    return new_labels\n",
    "\n",
    "def plot_cluster_sizes(labels, title):\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar(unique, counts, alpha=0.7)\n",
    "    plt.xlabel(\"Cluster ID\")\n",
    "    plt.ylabel(\"Size\")\n",
    "    plt.title(title)\n",
    "    plt.savefig(os.path.join(clusters_folder, f'{title}{with_scanNet_addition}.png'))\n",
    "    plt.close()\n",
    "\n",
    "def prevent_imbalanced_clusters(labels, max_ratio=0.3):\n",
    "    \"\"\"Reassigns clusters if one cluster is too large relative to others.\"\"\"\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    total = sum(counts)\n",
    "    max_size = total * max_ratio\n",
    "    if max(counts) > max_size:\n",
    "        print(\"Warning: Large cluster detected. Consider alternative clustering or additional constraints.\")\n",
    "    return labels\n",
    "def louvain_clustering(graph):\n",
    "    partition = community_louvain.best_partition(graph)\n",
    "    labels = np.array([partition[i] for i in range(len(partition))])\n",
    "    number_of_clusters = len(set(labels))\n",
    "    return number_of_clusters,labels\n",
    "\n",
    "def leiden_clustering(graph):\n",
    "    g = ig.Graph.Adjacency((nx.to_numpy_array(graph) > 0).tolist())\n",
    "    partition = find_partition(g, ModularityVertexPartition)\n",
    "    labels = np.zeros(graph.number_of_nodes(), dtype=int)\n",
    "    for i, cluster in enumerate(partition):\n",
    "        for node in cluster:\n",
    "            labels[node] = i\n",
    "    number_of_clusters = len(set(labels))\n",
    "    return number_of_clusters,labels\n",
    "\n",
    "def spectral_clustering(graph, n_clusters=80):\n",
    "    clustering = SpectralClustering(n_clusters=n_clusters, affinity='precomputed', assign_labels='kmeans')\n",
    "    labels = clustering.fit_predict(graphHomologous)\n",
    "    return n_clusters,labels\n",
    "\n",
    "def affinity_propagation(graph):\n",
    "    graph_dense = graph.toarray()  # Convert sparse matrix to dense numpy array\n",
    "    clustering = AffinityPropagation(affinity='precomputed')\n",
    "    labels = clustering.fit_predict(graph_dense)\n",
    "    number_of_clusters = len(set(labels))\n",
    "    return number_of_clusters,labels\n",
    "\n",
    "def connected_components_clustering(graph,directed=False):\n",
    "    \"\"\"Perform clustering using connected components.\"\"\"\n",
    "    n_components, labels = connected_components(csgraph=graph, directed=False, return_labels=True)\n",
    "    return n_components, labels\n",
    "\n",
    "def plot_graph_with_clusters(graph, labels, pos, title=\"Graph with Clusters\", top_n=10):\n",
    "    \"\"\"Plot the graph with nodes colored according to their cluster labels, only coloring the top N clusters.\"\"\"\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    sorted_indices = np.argsort(-counts)  # Sort in descending order\n",
    "    top_labels = unique_labels[sorted_indices[:top_n]]\n",
    "    \n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, top_n))  # Generate a color map for top N clusters\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for label, color in zip(top_labels, colors):\n",
    "        nx.draw_networkx_nodes(graph, pos, nodelist=[node for node, lbl in enumerate(labels) if lbl == label],\n",
    "                               node_color=[color], node_size=40, label=f\"Cluster {label}\")\n",
    "    nx.draw_networkx_nodes(graph, pos, nodelist=[node for node, lbl in enumerate(labels) if lbl not in top_labels],\n",
    "                           node_color='grey', node_size=20, label=\"Other Clusters\")  # Smaller grey circles\n",
    "    nx.draw_networkx_edges(graph, pos, alpha=0.5)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(clusters_folder, f'{title}_graph{with_scanNet_addition}.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert csr_matrix to NetworkX graph\n",
    "graph = nx.from_scipy_sparse_array(graphHomologous)\n",
    "pos = nx.spring_layout(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette score for Louvain clustering: 0.5212104211116715\n"
     ]
    }
   ],
   "source": [
    "_,labels_louvain = louvain_clustering(graph)\n",
    "labels_louvain = prevent_imbalanced_clusters(labels_louvain)\n",
    "labels_louvain = reorder_labels_by_cluster_size(labels_louvain)\n",
    "plot_cluster_sizes(labels_louvain, \"LouvainClustering\")\n",
    "plot_graph_with_clusters(graph, labels_louvain,pos, \"LouvainClustering\")\n",
    "silhouette_score_louvain = silhouette_score(graphHomologous, labels_louvain)\n",
    "print(f\"Silhouette score for Louvain clustering: {silhouette_score_louvain}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,labels_leiden = leiden_clustering(graph)\n",
    "labels_leiden = prevent_imbalanced_clusters(labels_leiden)\n",
    "labels_leiden = reorder_labels_by_cluster_size(labels_leiden)\n",
    "plot_cluster_sizes(labels_leiden, \"LeidenClustering\")\n",
    "plot_graph_with_clusters(graph, labels_leiden,pos, \"LeidenClustering\")\n",
    "silhouette_score_leiden = silhouette_score(graphHomologous, labels_leiden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iscb/wolfson/omriyakir/anaconda3/envs/experiment/lib/python3.10/site-packages/sklearn/manifold/_spectral_embedding.py:273: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Large cluster detected. Consider alternative clustering or additional constraints.\n",
      "Silhouette score for Spectral clustering: 0.4701041439848713\n"
     ]
    }
   ],
   "source": [
    "_,labels_spectral = spectral_clustering(graphHomologous)\n",
    "labels_spectral = prevent_imbalanced_clusters(labels_spectral)\n",
    "labels_spectral = reorder_labels_by_cluster_size(labels_spectral)\n",
    "plot_cluster_sizes(labels_spectral, \"SpectralClustering\")\n",
    "plot_graph_with_clusters(graph, labels_spectral,pos, \"SpectralClustering\")\n",
    "silhouette_score_spectral = silhouette_score(graphHomologous, labels_spectral)\n",
    "print(f\"Silhouette score for Spectral clustering: {silhouette_score_spectral}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iscb/wolfson/omriyakir/anaconda3/envs/experiment/lib/python3.10/site-packages/sklearn/cluster/_affinity_propagation.py:142: ConvergenceWarning: Affinity propagation did not converge, this model may return degenerate cluster centers and labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette score for Affinity propagation: 0.26185380811755826\n"
     ]
    }
   ],
   "source": [
    "_,labels_affinity = affinity_propagation(graphHomologous)\n",
    "labels_affinity = prevent_imbalanced_clusters(labels_affinity)\n",
    "labels_affinity = reorder_labels_by_cluster_size(labels_affinity)\n",
    "plot_cluster_sizes(labels_affinity, \"AffinityPropagation\")\n",
    "plot_graph_with_clusters(graph, labels_affinity,pos, \"AffinityPropagation\")\n",
    "silhouette_score_affinity = silhouette_score(graphHomologous, labels_affinity)\n",
    "print(f\"Silhouette score for Affinity propagation: {silhouette_score_affinity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Large cluster detected. Consider alternative clustering or additional constraints.\n",
      "Silhouette score for Connected components clustering: 0.2729209628793184\n"
     ]
    }
   ],
   "source": [
    "# make one for connected components\n",
    "_,labels_connected = connected_components_clustering(graphHomologous)\n",
    "labels_connected = prevent_imbalanced_clusters(labels_connected)\n",
    "labels_connected = reorder_labels_by_cluster_size(labels_connected)\n",
    "plot_cluster_sizes(labels_connected, \"ConnectedComponents\")\n",
    "plot_graph_with_clusters(graph, labels_connected,pos, \"ConnectedComponents\")\n",
    "silhouette_score_connected = silhouette_score(graphHomologous, labels_connected)\n",
    "print(f\"Silhouette score for Connected components clustering: {silhouette_score_connected}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'yaml' has no attribute 'error' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdata_preparation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mScanNet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcath_utils\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcath_utils\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata_preparation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mScanNet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdb_creation_scanNet_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save_as_pickle\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# dataset_date = \"8_9\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# seq_id = \"0.95\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# ASA_THRESHOLD_VALUE = 0.2\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# graph = nx.from_scipy_sparse_array(graphHomologous)\u001b[39;00m\n",
      "File \u001b[0;32m~/ubinet/data_preparation/ScanNet/../../data_preparation/ScanNet/cath_utils.py:11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcsgraph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m connected_components\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata_preparation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mScanNet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLabelPropagationAlgorithm_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m split_receptors_into_individual_chains_only_PSSM\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata_preparation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mScanNet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mAF2_augmentations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_chain_dict_with_all_info_only_pssm\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpaths\u001b[39;00m\n",
      "File \u001b[0;32m~/ubinet/data_preparation/ScanNet/../../data_preparation/ScanNet/LabelPropagationAlgorithm_utils.py:10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mScanNet_Ub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_FASTA, num2seq\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpdb\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata_preparation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mScanNet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdb_creation_scanNet_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save_as_pickle\n",
      "File \u001b[0;32m~/ubinet/data_preparation/ScanNet/../../models/ScanNet_Ub/preprocessing/sequence_utils.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prange, njit\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterpolate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interp1d\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/experiment/lib/python3.10/site-packages/numba/__init__.py:69\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m get_versions\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m generate_version_info\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types, errors\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Re-export typeof\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/experiment/lib/python3.10/site-packages/numba/core/config.py:10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# YAML needed to use file based Numba config\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01myaml\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     _HAVE_YAML \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/experiment/lib/python3.10/site-packages/yaml/__init__.py:13\u001b[0m\n\u001b[1;32m     11\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m6.0.2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcyaml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     14\u001b[0m     __with_libyaml__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/experiment/lib/python3.10/site-packages/yaml/cyaml.py:7\u001b[0m\n\u001b[1;32m      2\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCBaseLoader\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCSafeLoader\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCFullLoader\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUnsafeLoader\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCLoader\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCBaseDumper\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCSafeDumper\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCDumper\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      5\u001b[0m ]\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01myaml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_yaml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CParser, CEmitter\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstructor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mserializer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32myaml/_yaml.pyx:15\u001b[0m, in \u001b[0;36minit yaml._yaml\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'yaml' has no attribute 'error' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import data_preparation.ScanNet.cath_utils as cath_utils\n",
    "from data_preparation.ScanNet.db_creation_scanNet_utils import save_as_pickle\n",
    "\n",
    "# dataset_date = \"8_9\"\n",
    "# seq_id = \"0.95\"\n",
    "# ASA_THRESHOLD_VALUE = 0.2\n",
    "# with_scanNet = False\n",
    "# with_scanNet_addition = '_with_scanNet_' if with_scanNet else ''\n",
    "# os.makedirs(paths.cath_intermediate_files_path, exist_ok=True)\n",
    "    \n",
    "# pssm_folder = os.path.join(paths.PSSM_path,f'PSSM_{dataset_date}', f'seq_id_{seq_id}_asaThreshold_{ASA_THRESHOLD_VALUE}')\n",
    "# full_pssm_file_path = os.path.join(os.path.join(pssm_folder, f'propagatedPssmWithAsaFile_{seq_id}_asaThreshold_{ASA_THRESHOLD_VALUE}.txt'))\n",
    "# cath_df = cath_utils.make_cath_df_new(os.path.join(paths.cath_path, \"cath_b.20230204.txt\"))\n",
    "\n",
    "# if with_scanNet:\n",
    "#     scanNet_PSSM = cath_utils.scanNet_PSSM_files_concatenation()\n",
    "#     ubiq_output_PSSM, scanNet_output_PSSM = cath_utils.propagate_labels_and_concat(full_pssm_file_path,scanNet_PSSM,pssm_folder)\n",
    "#     full_pssm_file_path += with_scanNet_addition\n",
    "#     cath_utils.concat_and_remove_duplicates(ubiq_output_PSSM, scanNet_output_PSSM, full_pssm_file_path)\n",
    "\n",
    "# names_list, sizes_list, sequence_list, full_names_list, pdb_names_with_chains_lists = cath_utils.list_creation(\n",
    "#     full_pssm_file_path)\n",
    "\n",
    "    \n",
    "# structuresDicts = cath_utils.create_dictionaries(names_list, sizes_list, sequence_list, full_names_list,\n",
    "#                                                      pdb_names_with_chains_lists)\n",
    "# inCath, notInCath, cnt = cath_utils.count_in_cath(cath_df, structuresDicts)\n",
    "\n",
    "\n",
    "# cath_utils.find_chains_in_cath(cath_df, structuresDicts)\n",
    "# cath_utils.add_classifications_for_dict(cath_df, structuresDicts, 4)\n",
    "    \n",
    "# matHomologous_path = os.path.join(paths.cath_intermediate_files_path, f'matHomologous_{dataset_date}{with_scanNet_addition}.pkl')\n",
    "# graphHomologous_path = os.path.join(paths.cath_intermediate_files_path, f'graphHomologous_{dataset_date}{with_scanNet_addition}.pkl')\n",
    "# if os.path.exists(matHomologous_path) and os.path.exists(graphHomologous_path):\n",
    "#     matHomologous = cath_utils.load_from_pickle(matHomologous_path)\n",
    "#     graphHomologous = cath_utils.load_from_pickle(graphHomologous_path)\n",
    "# else:\n",
    "#     matHomologous = cath_utils.neighbor_mat_new(structuresDicts)\n",
    "#     graphHomologous = cath_utils.csr_matrix(matHomologous)\n",
    "#     save_as_pickle(matHomologous,matHomologous_path)\n",
    "#     save_as_pickle(graphHomologous,graphHomologous_path)\n",
    "    \n",
    "# graph = nx.from_scipy_sparse_array(graphHomologous)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data_preparation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata_preparation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mScanNet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcreate_tables_and_weights\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_table,add_model_num_to_dataset\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_components_for_algorithm\u001b[39m(algorithm_name,graph):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m algorithm_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlouvain\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'data_preparation'"
     ]
    }
   ],
   "source": [
    "from data_preparation.ScanNet.create_tables_and_weights import create_table,add_model_num_to_dataset\n",
    "def create_components_for_algorithm(algorithm_name,graph):\n",
    "    if algorithm_name == \"louvain\":\n",
    "        return louvain_clustering(graph)\n",
    "    elif algorithm_name == \"leiden\":\n",
    "        return leiden_clustering(graph)\n",
    "    elif algorithm_name == \"spectral\":\n",
    "        return spectral_clustering(graph)\n",
    "    elif algorithm_name == \"affinity\":\n",
    "        return affinity_propagation(graph)\n",
    "    elif algorithm_name == \"connected\":\n",
    "        return connected_components_clustering(graph)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown algorithm name: {algorithm_name}\")\n",
    "\n",
    "test_partition_folder = os.path.join(paths.cath_intermediate_files_path, f'test_partition{with_scanNet_addition}')\n",
    "os.mkdir(test_partition_folder, exist_ok=True)\n",
    "algorithm_names = [\"louvain\", \"leiden\", \"spectral\", \"affinity\", \"connected\"]\n",
    "results = {}\n",
    "for algorithm_name in algorithm_names:\n",
    "    algorithm_partition_folder = os.path.join(test_partition_folder, algorithm_name)\n",
    "    os.makedirs(algorithm_partition_folder, exist_ok=True)\n",
    "    n_clusters, labels = create_components_for_algorithm(algorithm_name,graphHomologous)\n",
    "    chainDict = cath_utils.components_to_chain_dict(n_clusters, labels,sizes_list,full_names_list)\n",
    "    cath_utils.divide_pssm(chainDict, full_pssm_file_path,algorithm_partition_folder,with_scanNet_addition)\n",
    "    for i in range(5):\n",
    "        pssm_file = os.path.join(algorithm_partition_folder, f\"PSSM{with_scanNet_addition}{str(i)}.txt\")\n",
    "        output_file = f'labels_fold{i+1}{with_scanNet_addition}.txt'\n",
    "        add_model_num_to_dataset(pssm_file, output_file)\n",
    "    create_table(algorithm_partition_folder ,algorithm_partition_folder,\"\",with_scanNet_addition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chainDict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mchainDict\u001b[49m\u001b[38;5;241m.\u001b[39mkeys()) \n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(chainDict)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# print(index_dict)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# def all_cross_edges(chainDict,structuresDicts):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#         index = index_dict[key]\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#         cluster = chainDict[key]\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chainDict' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "def calculate_max_fold_weight(file_path):\n",
    "    fold_weights = defaultdict(float)\n",
    "    total_weight = 0.0\n",
    "\n",
    "    with open(file_path, mode='r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            fold = row['Set']\n",
    "            weight = float(row['Sample weight'])\n",
    "            fold_weights[fold] += weight\n",
    "            total_weight += weight\n",
    "\n",
    "    max_fold_weight = max(fold_weights.values())\n",
    "    normalized_max_fold_weight = max_fold_weight / total_weight\n",
    "\n",
    "    return normalized_max_fold_weight\n",
    "\n",
    "def get_fold_dict(file_path):\n",
    "    fold_dict = {}\n",
    "\n",
    "    with open(file_path, mode='r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            pdb_id = row['PDB ID']\n",
    "            fold_number = int(row['Set'].split()[-1]) - 1\n",
    "            fold_dict[pdb_id] = fold_number\n",
    "\n",
    "    return fold_dict\n",
    "\n",
    "def get_index_dict(structuresDicts):\n",
    "    index_dict = {}\n",
    "    keys = list(structuresDicts.keys())\n",
    "    for i in range(len(keys)):\n",
    "        index_dict[keys[i]] = i\n",
    "\n",
    "    return index_dict\n",
    "\n",
    "def is_a_cross_edge(fold_dict,index_dict, id1, id2,matHomologous):\n",
    "    index1 = index_dict[id1]\n",
    "    index2 = index_dict[id2]\n",
    "    return matHomologous[index1][index2] == 1 and fold_dict[id1] != fold_dict[id2]\n",
    "\n",
    "\n",
    "def all_cross_edges(structuresDicts,table):\n",
    "    index_dict = get_index_dict(structuresDicts)\n",
    "    fold_dict = get_fold_dict(table)\n",
    "    print(f' len index_dict: {len(index_dict)}')\n",
    "    print(f' len fold_dict: {len(fold_dict)}')  \n",
    "     \n",
    "    cross_edges = 0\n",
    "    keys = list(fold_dict.keys())\n",
    "    for i in range(len(keys)):\n",
    "        for j in range(i + 1, len(keys)):\n",
    "            id1 = keys[i]\n",
    "            id2 = keys[j]\n",
    "            if is_a_cross_edge(fold_dict, index_dict, id1, id2, matHomologous):\n",
    "                cross_edges += 1\n",
    "\n",
    "    return cross_edges\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Max Fold Weight: 0.4092827004219409\n",
      " len index_dict: 468\n",
      " len fold_dict: 462\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "file_path = '/home/iscb/wolfson/omriyakir/ubinet/datasets/scanNet/data_for_training/8_9_dataset/seq_id_0.9_asaThreshold_0.1/table.csv'\n",
    "normalized_max_fold_weight = calculate_max_fold_weight(file_path)\n",
    "print(f'Normalized Max Fold Weight: {normalized_max_fold_weight}')\n",
    "print(all_cross_edges(structuresDicts,file_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "experiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
