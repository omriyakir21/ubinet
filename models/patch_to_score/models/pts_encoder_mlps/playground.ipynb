{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6d31464",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 15:17:42.247289: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-09 15:17:42.368931: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-09 15:17:49.702906: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/iscb/wolfson/doririmon/anaconda3/envs/ubinet-gpu/lib\n",
      "2025-04-09 15:17:49.703010: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/iscb/wolfson/doririmon/anaconda3/envs/ubinet-gpu/lib\n",
      "2025-04-09 15:17:49.703020: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f7ae7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb512325",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7751d25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_21\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_11 (InputLayer)       [(None, 11)]              0         \n",
      "                                                                 \n",
      " dense_185 (Dense)           (None, 1024)              12288     \n",
      "                                                                 \n",
      " transformer_encoder_mlp_23   (None, 1024)             2101248   \n",
      " (TransformerEncoderMLP)                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,113,536\n",
      "Trainable params: 2,113,536\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Example input\n",
    "inputs = tf.keras.Input(shape=(11,))\n",
    "x = tf.keras.layers.Dense(1024, activation='relu')(inputs)\n",
    "x = TransformerEncoderMLP(hidden_units=(1024, 1024), dropout_rate=0.2)(x)\n",
    "model = tf.keras.Model(inputs, x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a6788577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 1024), dtype=float32, numpy=\n",
       "array([[ 0.90425795,  0.21220875,  0.12675744, ..., -1.2134382 ,\n",
       "        -0.41431978, -1.2155956 ],\n",
       "       [ 2.0990896 ,  0.42351112,  0.23918264, ..., -1.4000756 ,\n",
       "         0.7024706 , -1.4182214 ],\n",
       "       [ 0.19723973, -0.4140548 ,  0.6895801 , ..., -0.9461419 ,\n",
       "         0.39200586, -1.8634183 ],\n",
       "       ...,\n",
       "       [-0.07631502, -0.48998123, -0.30358535, ..., -1.0339234 ,\n",
       "        -0.07782497, -0.83783656],\n",
       "       [-0.14333615, -1.0692644 , -0.4346997 , ..., -1.1219832 ,\n",
       "        -0.1697183 , -0.93750757],\n",
       "       [-0.12023279, -0.729512  ,  0.15584618, ..., -0.992909  ,\n",
       "         0.914152  , -0.83675957]], dtype=float32)>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(tf.random.uniform((10, 11)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8c2f7aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.patch_to_score.models.pts_encoder_mlps.utils import GlobalSumPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7d5676db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "# import tensorflow as tf\n",
    "# from models.patch_to_score.models.pts_without_mlp_b.utils import GlobalSumPooling\n",
    "\n",
    "\n",
    "def broadcast_shape(x, max_number_of_patches: int) -> tf.Tensor:\n",
    "    x_expanded = tf.expand_dims(x, axis=1)\n",
    "    x_broadcasted = tf.broadcast_to(x_expanded, [tf.shape(x)[0], max_number_of_patches, tf.shape(x)[-1]])\n",
    "    return x_broadcasted\n",
    "\n",
    "\n",
    "def build_model_prev(m_a: int, m_c: int, n_layers: int, input_shape: Tuple[int, int],\n",
    "                max_number_of_patches: int) -> tf.keras.models.Model:\n",
    "    '''\n",
    "    :param m_a: size of the hidden layers in the MLP of the components\n",
    "    :param m_c: size of the hidden layers in the MLP of the concatenated global sum output and size + n_patches MLP output\n",
    "    :param n_layers: number of layers in each of the MLPs\n",
    "    :param input_shape: shape of the input data (number of patches, number of features)\n",
    "    :param max_number_of_patches: maximum number of patches\n",
    "    :return: a Keras model\n",
    "    '''\n",
    "    input_data = tf.keras.Input(shape=input_shape, name='patches_input')\n",
    "    size_value = tf.keras.Input(shape=(1,), name='extra_value_input')\n",
    "    n_patches_hot_encoded_value = tf.keras.Input(\n",
    "        shape=(max_number_of_patches + 1,), name='hot_encoded_value_input')\n",
    "    n_patches = tf.argmax(n_patches_hot_encoded_value, axis=1)[..., None]\n",
    "    n_patches = tf.cast(n_patches, tf.float32)\n",
    "    \n",
    "    n_patches_broadcased = broadcast_shape(n_patches, max_number_of_patches)\n",
    "    size_broadcased = broadcast_shape(size_value, max_number_of_patches)\n",
    "    \n",
    "    concat_input_data = tf.keras.layers.Concatenate(axis=-1)([input_data, n_patches_broadcased, size_broadcased])\n",
    "    masked_input = tf.keras.layers.Masking(mask_value=0.0)(concat_input_data)\n",
    "\n",
    "    currentOutput = masked_input\n",
    "    for i in range(n_layers):\n",
    "        dense_output = tf.keras.layers.Dense(\n",
    "            m_a, activation='linear')(currentOutput)\n",
    "        batchNorm = tf.keras.layers.BatchNormalization(\n",
    "            momentum=0.75)(dense_output)\n",
    "        activation = tf.keras.layers.ReLU()(batchNorm)\n",
    "        currentOutput = activation\n",
    "\n",
    "\n",
    "    print('before global pooling', currentOutput)\n",
    "    global_pooling_output = GlobalSumPooling(\n",
    "        data_format='channels_last')(currentOutput)\n",
    "    print('after global pooling shape:', global_pooling_output)\n",
    "\n",
    "    currentOutput = global_pooling_output\n",
    "    for i in range(n_layers):\n",
    "        dense_output = tf.keras.layers.Dense(\n",
    "            m_c, activation='linear')(currentOutput)\n",
    "        batchNorm = tf.keras.layers.BatchNormalization(\n",
    "            momentum=0.75)(dense_output)\n",
    "        activation = tf.keras.layers.ReLU()(batchNorm)\n",
    "        currentOutput = activation\n",
    "\n",
    "    before_sigmoid_output = currentOutput\n",
    "\n",
    "    output = tf.keras.layers.Dense(\n",
    "        1, activation='sigmoid')(before_sigmoid_output)\n",
    "    model = tf.keras.Model(\n",
    "        inputs=[input_data, size_value, n_patches_hot_encoded_value], outputs=output)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a014dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "import tensorflow as tf\n",
    "from models.patch_to_score.models.pts_encoder_mlps.transformer_encoder_mlp import TransformerEncoderMLP\n",
    "\n",
    "\n",
    "def broadcast_shape(x, max_number_of_patches: int) -> tf.Tensor:\n",
    "    x_expanded = tf.expand_dims(x, axis=1)\n",
    "    x_broadcasted = tf.broadcast_to(x_expanded, [tf.shape(x)[0], max_number_of_patches, tf.shape(x)[-1]])\n",
    "    return x_broadcasted\n",
    "\n",
    "\n",
    "def build_model(hidden_sizes_mlp_a: List[Tuple[int, int]], mlp_a_dropout_rate: float,\n",
    "                hidden_sizes_mlp_c: List[Tuple[int, int]], mlp_c_dropout_rate: float,\n",
    "                activation: str,\n",
    "                input_shape: Tuple[int, int],\n",
    "                max_number_of_patches: int) -> tf.keras.models.Model:\n",
    "    '''\n",
    "    :param m_a: size of the hidden layers in the MLP of the components\n",
    "    :param m_c: size of the hidden layers in the MLP of the concatenated global sum output and size + n_patches MLP output\n",
    "    :param n_layers: number of layers in each of the MLPs\n",
    "    :param input_shape: shape of the input data (number of patches, number of features)\n",
    "    :param max_number_of_patches: maximum number of patches\n",
    "    :return: a Keras model\n",
    "    '''\n",
    "    input_data = tf.keras.Input(shape=input_shape, name='patches_input')\n",
    "    size_value = tf.keras.Input(shape=(1,), name='extra_value_input')\n",
    "    n_patches_hot_encoded_value = tf.keras.Input(\n",
    "        shape=(max_number_of_patches + 1,), name='hot_encoded_value_input')\n",
    "    n_patches = tf.argmax(n_patches_hot_encoded_value, axis=1)[..., None]\n",
    "    n_patches = tf.cast(n_patches, tf.float32)\n",
    "    \n",
    "    n_patches_broadcased = broadcast_shape(n_patches, max_number_of_patches)\n",
    "    size_broadcased = broadcast_shape(size_value, max_number_of_patches)\n",
    "    \n",
    "    concat_input_data = tf.keras.layers.Concatenate(axis=-1)([input_data, n_patches_broadcased, size_broadcased])\n",
    "    masked_input = tf.keras.layers.Masking(mask_value=0.0)(concat_input_data)\n",
    "    \n",
    "    current_output = masked_input\n",
    "    print(current_output.shape)\n",
    "    current_output = tf.keras.layers.Dense(\n",
    "        hidden_sizes_mlp_a[0][0], activation='linear')(current_output)\n",
    "    \n",
    "    for mlp_hidden_size in hidden_sizes_mlp_a:\n",
    "        mlp = TransformerEncoderMLP(\n",
    "            hidden_units=mlp_hidden_size, dropout_rate=mlp_a_dropout_rate, activation=activation)\n",
    "        current_output = mlp(current_output)\n",
    "        \n",
    "        # current_output = apply_transformer_mlp(current_output, mlp_hidden_size, mlp_a_dropout_rate, activation)\n",
    "        # activation = tf.keras.layers.ReLU()(current_output)\n",
    "        # current_output = activation\n",
    "        # TODO: apply non-linearity here?\n",
    "    \n",
    "    print('before global pooling', current_output)\n",
    "    \n",
    "    global_pooling_output = GlobalSumPooling(\n",
    "        data_format='channels_last')(current_output)\n",
    "\n",
    "    print('after global pooling:', global_pooling_output)\n",
    "    \n",
    "    current_output = global_pooling_output\n",
    "    current_output = tf.keras.layers.Dense(\n",
    "        hidden_sizes_mlp_c[0][0], activation='linear')(current_output)\n",
    "    \n",
    "    print('here')\n",
    "    for mlp_hidden_size in hidden_sizes_mlp_c:\n",
    "        mlp = TransformerEncoderMLP(\n",
    "            hidden_units=mlp_hidden_size, dropout_rate=mlp_c_dropout_rate, activation=activation)\n",
    "        current_output = mlp(current_output)\n",
    "        \n",
    "        # current_output = apply_transformer_mlp(current_output, mlp_hidden_size, mlp_a_dropout_rate, activation)\n",
    "        # TODO: apply non-linearity here?\n",
    "\n",
    "    before_sigmoid_output = current_output\n",
    "\n",
    "    output = tf.keras.layers.Dense(\n",
    "        1, activation='sigmoid')(before_sigmoid_output)\n",
    "    model = tf.keras.Model(\n",
    "        inputs=[input_data, size_value, n_patches_hot_encoded_value], outputs=output)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d52445e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_pts_encoder_mlps(hidden_sizes_mlp_a: List[Tuple[int, int]], mlp_a_dropout_rate: float,\n",
    "                               hidden_sizes_mlp_c: List[Tuple[int, int]], mlp_c_dropout_rate: float,\n",
    "                               activation: str, input_shape: Tuple[int, int],\n",
    "                               max_number_of_patches: int) -> tf.keras.models.Model:\n",
    "    model = build_model(hidden_sizes_mlp_a, mlp_a_dropout_rate,\n",
    "                                         hidden_sizes_mlp_c, mlp_c_dropout_rate,\n",
    "                                         activation, input_shape, max_number_of_patches)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8d91ffc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before global pooling KerasTensor(type_spec=TensorSpec(shape=(None, 10, 1024), dtype=tf.float32, name=None), name='re_lu_94/Relu:0', description=\"created by layer 're_lu_94'\")\n",
      "mask is not None\n",
      "after global pooling shape: KerasTensor(type_spec=TensorSpec(shape=(None, 1024), dtype=tf.float32, name=None), name='global_sum_pooling_17/Sum:0', description=\"created by layer 'global_sum_pooling_17'\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x7279355e5ae0>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_model_prev(1024, 1024, 4, [10, 9], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "31eecd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 10, 11)\n",
      "before global pooling KerasTensor(type_spec=TensorSpec(shape=(None, 10, 1024), dtype=tf.float32, name=None), name='transformer_encoder_mlp_25/layer_normalization_31/add:0', description=\"created by layer 'transformer_encoder_mlp_25'\")\n",
      "mask is not None\n",
      "after global pooling: KerasTensor(type_spec=TensorSpec(shape=(None, 1024), dtype=tf.float32, name=None), name='global_sum_pooling_18/Sum:0', description=\"created by layer 'global_sum_pooling_18'\")\n",
      "here\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x7277f63e18a0>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_model(\n",
    "    hidden_sizes_mlp_a=[(1024, 1024), (1024, 1024)],\n",
    "    mlp_a_dropout_rate=0.2,\n",
    "    hidden_sizes_mlp_c=[(1024, 1024), (1024, 1024)],\n",
    "    mlp_c_dropout_rate=0.2,\n",
    "    activation='relu',\n",
    "    input_shape=[10, 9],\n",
    "    max_number_of_patches=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3c4a89af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrap_pts_encoder_mlps(\n",
    "#     hidden_sizes_mlp_a=[(1024, 1024), (1024, 1024)],\n",
    "#     mlp_a_dropout_rate=0.2,\n",
    "#     hidden_sizes_mlp_c=[(1024, 1024), (1024, 1024)],\n",
    "#     mlp_c_dropout_rate=0.2,\n",
    "#     activation='relu',\n",
    "#     input_shape=[10, 9],\n",
    "#     max_number_of_patches=10\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ubinet-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
