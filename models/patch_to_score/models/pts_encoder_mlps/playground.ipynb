{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6d31464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f7ae7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb512325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 21:56:16.888132: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2027] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 9.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n",
      "2025-04-06 21:56:16.889441: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2027] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 9.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n",
      "2025-04-06 21:56:16.891218: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2027] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 9.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n",
      "2025-04-06 21:56:16.892819: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2027] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 9.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n",
      "2025-04-06 21:56:16.894026: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2027] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 9.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n",
      "2025-04-06 21:56:16.895169: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2027] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 9.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n",
      "2025-04-06 21:56:16.896442: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2027] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 9.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n",
      "2025-04-06 21:56:16.897589: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2027] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 9.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n"
     ]
    }
   ],
   "source": [
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6b4839c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_encoder_mlp import TransformerEncoderMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7751d25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 11)]              0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 1024)              12288     \n",
      "                                                                 \n",
      " transformer_encoder_mlp_7 (  (None, 1024)             2101248   \n",
      " TransformerEncoderMLP)                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,113,536\n",
      "Trainable params: 2,113,536\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Example input\n",
    "inputs = tf.keras.Input(shape=(11,))\n",
    "x = tf.keras.layers.Dense(1024, activation='relu')(inputs)\n",
    "x = TransformerEncoderMLP(hidden_units=(1024, 1024), dropout_rate=0.2)(x)\n",
    "model = tf.keras.Model(inputs, x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6788577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 1024), dtype=float32, numpy=\n",
       "array([[-0.6960208 , -1.5768466 ,  0.07993226, ..., -0.14368364,\n",
       "         2.5021944 , -0.41295868],\n",
       "       [-0.13876311, -1.4246821 ,  0.41824263, ...,  0.49558175,\n",
       "         3.446252  ,  0.28421518],\n",
       "       [-1.5456738 , -1.8446131 ,  0.3026702 , ..., -0.43714157,\n",
       "         2.3914535 , -0.7879347 ],\n",
       "       ...,\n",
       "       [-0.6925067 , -1.2031287 , -0.5697024 , ...,  0.6399677 ,\n",
       "         0.55358505,  0.70341927],\n",
       "       [-0.7889751 , -1.3827075 , -0.33191642, ...,  0.24981944,\n",
       "         1.8018522 , -0.11736801],\n",
       "       [-0.41686153, -1.5389943 , -0.41158846, ...,  0.24025422,\n",
       "         2.9066348 ,  0.1695547 ]], dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(tf.random.uniform((10, 11)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7d5676db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import tensorflow as tf\n",
    "from models.patch_to_score.models.pts_without_mlp_b.utils import GlobalSumPooling\n",
    "\n",
    "\n",
    "def broadcast_shape(x, max_number_of_patches: int) -> tf.Tensor:\n",
    "    x_expanded = tf.expand_dims(x, axis=1)\n",
    "    x_broadcasted = tf.broadcast_to(x_expanded, [tf.shape(x)[0], max_number_of_patches, tf.shape(x)[-1]])\n",
    "    return x_broadcasted\n",
    "\n",
    "\n",
    "def build_model_prev(m_a: int, m_c: int, n_layers: int, input_shape: Tuple[int, int],\n",
    "                max_number_of_patches: int) -> tf.keras.models.Model:\n",
    "    '''\n",
    "    :param m_a: size of the hidden layers in the MLP of the components\n",
    "    :param m_c: size of the hidden layers in the MLP of the concatenated global sum output and size + n_patches MLP output\n",
    "    :param n_layers: number of layers in each of the MLPs\n",
    "    :param input_shape: shape of the input data (number of patches, number of features)\n",
    "    :param max_number_of_patches: maximum number of patches\n",
    "    :return: a Keras model\n",
    "    '''\n",
    "    input_data = tf.keras.Input(shape=input_shape, name='patches_input')\n",
    "    size_value = tf.keras.Input(shape=(1,), name='extra_value_input')\n",
    "    n_patches_hot_encoded_value = tf.keras.Input(\n",
    "        shape=(max_number_of_patches + 1,), name='hot_encoded_value_input')\n",
    "    n_patches = tf.argmax(n_patches_hot_encoded_value, axis=1)[..., None]\n",
    "    n_patches = tf.cast(n_patches, tf.float32)\n",
    "    \n",
    "    n_patches_broadcased = broadcast_shape(n_patches, max_number_of_patches)\n",
    "    size_broadcased = broadcast_shape(size_value, max_number_of_patches)\n",
    "    \n",
    "    concat_input_data = tf.keras.layers.Concatenate(axis=-1)([input_data, n_patches_broadcased, size_broadcased])\n",
    "    masked_input = tf.keras.layers.Masking(mask_value=0.0)(concat_input_data)\n",
    "\n",
    "    currentOutput = masked_input\n",
    "    for i in range(n_layers):\n",
    "        dense_output = tf.keras.layers.Dense(\n",
    "            m_a, activation='linear')(currentOutput)\n",
    "        batchNorm = tf.keras.layers.BatchNormalization(\n",
    "            momentum=0.75)(dense_output)\n",
    "        activation = tf.keras.layers.ReLU()(batchNorm)\n",
    "        currentOutput = activation\n",
    "\n",
    "\n",
    "    print('before global pooling shape:', currentOutput.shape)\n",
    "    global_pooling_output = GlobalSumPooling(\n",
    "        data_format='channels_last')(currentOutput)\n",
    "    print('after global pooling shape:', global_pooling_output.shape)\n",
    "\n",
    "    currentOutput = global_pooling_output\n",
    "    for i in range(n_layers):\n",
    "        dense_output = tf.keras.layers.Dense(\n",
    "            m_c, activation='linear')(currentOutput)\n",
    "        batchNorm = tf.keras.layers.BatchNormalization(\n",
    "            momentum=0.75)(dense_output)\n",
    "        activation = tf.keras.layers.ReLU()(batchNorm)\n",
    "        currentOutput = activation\n",
    "\n",
    "    before_sigmoid_output = currentOutput\n",
    "\n",
    "    output = tf.keras.layers.Dense(\n",
    "        1, activation='sigmoid')(before_sigmoid_output)\n",
    "    model = tf.keras.Model(\n",
    "        inputs=[input_data, size_value, n_patches_hot_encoded_value], outputs=output)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7a014dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "import tensorflow as tf\n",
    "# from models.patch_to_score.models.pts_encoder_mlps.utils import GlobalSumPooling\n",
    "from models.patch_to_score.models.pts_encoder_mlps.transformer_encoder_mlp import TransformerEncoderMLP\n",
    "\n",
    "\n",
    "def broadcast_shape(x, max_number_of_patches: int) -> tf.Tensor:\n",
    "    x_expanded = tf.expand_dims(x, axis=1)\n",
    "    x_broadcasted = tf.broadcast_to(x_expanded, [tf.shape(x)[0], max_number_of_patches, tf.shape(x)[-1]])\n",
    "    return x_broadcasted\n",
    "\n",
    "\n",
    "def build_model(hidden_sizes_mlp_a: List[Tuple[int, int]], mlp_a_dropout_rate: float,\n",
    "                hidden_sizes_mlp_c: List[Tuple[int, int]], mlp_c_dropout_rate: float,\n",
    "                activation: str,\n",
    "                input_shape: Tuple[int, int],\n",
    "                max_number_of_patches: int) -> tf.keras.models.Model:\n",
    "    '''\n",
    "    :param m_a: size of the hidden layers in the MLP of the components\n",
    "    :param m_c: size of the hidden layers in the MLP of the concatenated global sum output and size + n_patches MLP output\n",
    "    :param n_layers: number of layers in each of the MLPs\n",
    "    :param input_shape: shape of the input data (number of patches, number of features)\n",
    "    :param max_number_of_patches: maximum number of patches\n",
    "    :return: a Keras model\n",
    "    '''\n",
    "    input_data = tf.keras.Input(shape=input_shape, name='patches_input')\n",
    "    size_value = tf.keras.Input(shape=(1,), name='extra_value_input')\n",
    "    n_patches_hot_encoded_value = tf.keras.Input(\n",
    "        shape=(max_number_of_patches + 1,), name='hot_encoded_value_input')\n",
    "    n_patches = tf.argmax(n_patches_hot_encoded_value, axis=1)[..., None]\n",
    "    n_patches = tf.cast(n_patches, tf.float32)\n",
    "    \n",
    "    n_patches_broadcased = broadcast_shape(n_patches, max_number_of_patches)\n",
    "    size_broadcased = broadcast_shape(size_value, max_number_of_patches)\n",
    "    \n",
    "    concat_input_data = tf.keras.layers.Concatenate(axis=-1)([input_data, n_patches_broadcased, size_broadcased])\n",
    "    masked_input = tf.keras.layers.Masking(mask_value=0.0)(concat_input_data)\n",
    "    \n",
    "    current_output = masked_input\n",
    "    print(current_output.shape)\n",
    "    current_output = tf.keras.layers.Dense(\n",
    "        hidden_sizes_mlp_a[0][0], activation='linear')(current_output)\n",
    "    \n",
    "    for mlp_hidden_size in hidden_sizes_mlp_a:\n",
    "        mlp = TransformerEncoderMLP(\n",
    "            hidden_units=mlp_hidden_size, dropout_rate=mlp_a_dropout_rate, activation=activation)\n",
    "        current_output = mlp(current_output)\n",
    "        activation = tf.keras.layers.ReLU()(current_output)\n",
    "        current_output = activation\n",
    "        # TODO: apply non-linearity here?\n",
    "    \n",
    "    print('before global pool shape', current_output.shape)\n",
    "    \n",
    "    global_pooling_output = GlobalSumPooling(\n",
    "        data_format='channels_last')(current_output)\n",
    "\n",
    "    print('after global pooling shape', global_pooling_output.shape)\n",
    "    current_output = global_pooling_output\n",
    "    current_output = tf.keras.layers.Dense(\n",
    "        hidden_sizes_mlp_c[0][0], activation='linear')(current_output)\n",
    "    \n",
    "    print('here')\n",
    "    for mlp_hidden_size in hidden_sizes_mlp_c:\n",
    "        mlp = TransformerEncoderMLP(\n",
    "            hidden_units=mlp_hidden_size, dropout_rate=mlp_c_dropout_rate, activation=activation)\n",
    "        current_output = mlp(current_output)\n",
    "        # TODO: apply non-linearity here?\n",
    "\n",
    "    before_sigmoid_output = current_output\n",
    "\n",
    "    output = tf.keras.layers.Dense(\n",
    "        1, activation='sigmoid')(before_sigmoid_output)\n",
    "    model = tf.keras.Model(\n",
    "        inputs=[input_data, size_value, n_patches_hot_encoded_value], outputs=output)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d52445e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_pts_encoder_mlps(hidden_sizes_mlp_a: List[Tuple[int, int]], mlp_a_dropout_rate: float,\n",
    "                               hidden_sizes_mlp_c: List[Tuple[int, int]], mlp_c_dropout_rate: float,\n",
    "                               activation: str, input_shape: Tuple[int, int],\n",
    "                               max_number_of_patches: int) -> tf.keras.models.Model:\n",
    "    model = build_model(hidden_sizes_mlp_a, mlp_a_dropout_rate,\n",
    "                                         hidden_sizes_mlp_c, mlp_c_dropout_rate,\n",
    "                                         activation, input_shape, max_number_of_patches)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8d91ffc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before global pooling shape: (None, 10, 1024)\n",
      "after global pooling shape: (None, 1024)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x735365dede40>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_model_prev(1024, 1024, 2, [10, 9], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3c4a89af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 10, 11)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Exception encountered when calling layer \"tf.keras.activations.get_1\" (type TFOpLambda).\n\nCould not interpret activation function identifier: Tensor(\"Placeholder:0\", shape=(None, 10, 1024), dtype=float32)\n\nCall arguments received by layer \"tf.keras.activations.get_1\" (type TFOpLambda):\n  • identifier=tf.Tensor(shape=(None, 10, 1024), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[110], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbootstrap_pts_encoder_mlps\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_sizes_mlp_a\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmlp_a_dropout_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_sizes_mlp_c\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmlp_c_dropout_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_number_of_patches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[108], line 5\u001b[0m, in \u001b[0;36mbootstrap_pts_encoder_mlps\u001b[0;34m(hidden_sizes_mlp_a, mlp_a_dropout_rate, hidden_sizes_mlp_c, mlp_c_dropout_rate, activation, input_shape, max_number_of_patches)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbootstrap_pts_encoder_mlps\u001b[39m(hidden_sizes_mlp_a: List[Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]], mlp_a_dropout_rate: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[1;32m      2\u001b[0m                                hidden_sizes_mlp_c: List[Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]], mlp_c_dropout_rate: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[1;32m      3\u001b[0m                                activation: \u001b[38;5;28mstr\u001b[39m, input_shape: Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m],\n\u001b[1;32m      4\u001b[0m                                max_number_of_patches: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mModel:\n\u001b[0;32m----> 5\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_sizes_mlp_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlp_a_dropout_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mhidden_sizes_mlp_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlp_c_dropout_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_number_of_patches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "Cell \u001b[0;32mIn[107], line 45\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m(hidden_sizes_mlp_a, mlp_a_dropout_rate, hidden_sizes_mlp_c, mlp_c_dropout_rate, activation, input_shape, max_number_of_patches)\u001b[0m\n\u001b[1;32m     41\u001b[0m current_output \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\n\u001b[1;32m     42\u001b[0m     hidden_sizes_mlp_a[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m], activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m)(current_output)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mlp_hidden_size \u001b[38;5;129;01min\u001b[39;00m hidden_sizes_mlp_a:\n\u001b[0;32m---> 45\u001b[0m     mlp \u001b[38;5;241m=\u001b[39m \u001b[43mTransformerEncoderMLP\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_units\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmlp_hidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmlp_a_dropout_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     current_output \u001b[38;5;241m=\u001b[39m mlp(current_output)\n\u001b[1;32m     48\u001b[0m     activation \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mReLU()(current_output)\n",
      "File \u001b[0;32m~/home/order/ubinet/repo/ubinet/models/patch_to_score/models/pts_encoder_mlps/../../../../models/patch_to_score/models/pts_encoder_mlps/transformer_encoder_mlp.py:21\u001b[0m, in \u001b[0;36mTransformerEncoderMLP.__init__\u001b[0;34m(self, hidden_units, dropout_rate, activation, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation \u001b[38;5;241m=\u001b[39m activation\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense1 \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mDense(hidden_units[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_layer \u001b[38;5;241m=\u001b[39m \u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mActivation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1 \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mDropout(dropout_rate)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense2 \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mDense(hidden_units[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/ubinet-gpu/lib/python3.10/site-packages/keras/layers/core/activation.py:56\u001b[0m, in \u001b[0;36mActivation.__init__\u001b[0;34m(self, activation, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_masking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation \u001b[38;5;241m=\u001b[39m \u001b[43mactivations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ubinet-gpu/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/ubinet-gpu/lib/python3.10/site-packages/keras/layers/core/tf_op_layer.py:119\u001b[0m, in \u001b[0;36mKerasOpDispatcher.handle\u001b[0;34m(self, op, args, kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Handle the specified operation with the specified arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(x, keras_tensor\u001b[38;5;241m.\u001b[39mKerasTensor)\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten([args, kwargs])\n\u001b[1;32m    118\u001b[0m ):\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTFOpLambda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNOT_SUPPORTED\n",
      "File \u001b[0;32m~/anaconda3/envs/ubinet-gpu/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/ubinet-gpu/lib/python3.10/site-packages/keras/activations.py:613\u001b[0m, in \u001b[0;36mget\u001b[0;34m(identifier)\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m identifier\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 613\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    614\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not interpret activation function identifier: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midentifier\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    615\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: Exception encountered when calling layer \"tf.keras.activations.get_1\" (type TFOpLambda).\n\nCould not interpret activation function identifier: Tensor(\"Placeholder:0\", shape=(None, 10, 1024), dtype=float32)\n\nCall arguments received by layer \"tf.keras.activations.get_1\" (type TFOpLambda):\n  • identifier=tf.Tensor(shape=(None, 10, 1024), dtype=float32)"
     ]
    }
   ],
   "source": [
    "bootstrap_pts_encoder_mlps(\n",
    "    hidden_sizes_mlp_a=[(1024, 1024), (1024, 1024)],\n",
    "    mlp_a_dropout_rate=0.2,\n",
    "    hidden_sizes_mlp_c=[(1024, 1024), (1024, 1024)],\n",
    "    mlp_c_dropout_rate=0.2,\n",
    "    activation='relu',\n",
    "    input_shape=[10, 9],\n",
    "    max_number_of_patches=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b94288",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ubinet-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
